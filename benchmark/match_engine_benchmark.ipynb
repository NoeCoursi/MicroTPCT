{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MicroTPCT – Matching Algorithm Benchmark Notebook\n",
    "\n",
    "This notebook benchmarks several exact peptide-to-proteome matching algorithms implemented in MicroTPCT.  \n",
    "We evaluate execution time, CPU usage, peak memory consumption, and correctness in order to guide default algorithm selection and user-level configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This benchmark evaluates several exact peptide-to-proteome matching strategies implemented in MicroTPCT.\n",
    "\n",
    "The objectives are:\n",
    "\n",
    "- Compare execution time, CPU usage, and peak memory consumption across multiple algorithms.\n",
    "- Verify correctness against a reference method.\n",
    "- Characterize algorithmic behavior under different scenarios.\n",
    "- Provide practical guidance for default algorithm selection and user-level configuration.\n",
    "\n",
    "The following algorithms are compared:\n",
    "\n",
    "- Naive Python baseline (`str.find`) (reference method)\n",
    "- Boyer–Moore\n",
    "- Aho–Corasick (pure Python implementation)\n",
    "- Aho–Corasick (Rust backend)\n",
    "- System-level `grep + awk` launcher\n",
    "- BLAST (included as a qualitative reference, not as a competitive exact matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and configuration\n",
    "\n",
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from microtpct.utils.data_generator import generate_benchmark_databases\n",
    "from microtpct.core.match import get_engine, list_available_engines\n",
    "from microtpct.io.writers import _sanitize_name\n",
    "\n",
    "from benchmark_core import BenchmarkResult, run_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 System and execution environment\n",
    "\n",
    "In order to ensure reproducibility and allow proper interpretation of performance results, we report the hardware and software environment used for this benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform, sys, psutil\n",
    "\n",
    "print(\"System:\", platform.platform())\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"CPU:\", platform.processor())\n",
    "print(\"Cores:\", psutil.cpu_count(logical=True))\n",
    "print(\"RAM (GB):\", round(psutil.virtual_memory().total / 1e9, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Benchmark scenarios\n",
    "\n",
    "We define a limited set of synthetic scenarios designed to expose algorithmic differences without performing an exhaustive grid search.\n",
    "\n",
    "Each scenario controls:\n",
    "\n",
    "- Proteome size and length distribution  \n",
    "- Number and length of peptides  \n",
    "- Match density  \n",
    "- Sequence redundancy  \n",
    "\n",
    "These scenarios aim to reproduce both realistic use cases and stress conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARIOS = {\n",
    "    \"small_sparse\": dict(\n",
    "        n_proteins=1000,\n",
    "        protein_mean_length=300,\n",
    "        protein_std_length=50,\n",
    "        x_rate=0.0,\n",
    "        n_peptides=1000,\n",
    "        peptide_mean_length=10,\n",
    "        peptide_std_length=2,\n",
    "        match_fraction=0.05,\n",
    "        quasi_fraction=0.0,\n",
    "        redundancy_rate=0.0,\n",
    "        mutation_rate=0.0,\n",
    "        seed=1,\n",
    "    ),\n",
    "\n",
    "    \"many_queries\": dict(\n",
    "        n_proteins=5000,\n",
    "        protein_mean_length=300,\n",
    "        protein_std_length=50,\n",
    "        x_rate=0.0,\n",
    "        n_peptides=20000,\n",
    "        peptide_mean_length=10,\n",
    "        peptide_std_length=2,\n",
    "        match_fraction=0.1,\n",
    "        quasi_fraction=0.0,\n",
    "        redundancy_rate=0.0,\n",
    "        mutation_rate=0.0,\n",
    "        seed=2,\n",
    "    ),\n",
    "\n",
    "    \"short_peptides\": dict(\n",
    "        n_proteins=2000,\n",
    "        protein_mean_length=300,\n",
    "        protein_std_length=50,\n",
    "        x_rate=0.0,\n",
    "        n_peptides=3000,\n",
    "        peptide_mean_length=6,\n",
    "        peptide_std_length=1,\n",
    "        match_fraction=0.3,\n",
    "        quasi_fraction=0.0,\n",
    "        redundancy_rate=0.0,\n",
    "        mutation_rate=0.0,\n",
    "        seed=3,\n",
    "    ),\n",
    "\n",
    "    \"negative_only\": dict(\n",
    "        n_proteins=5000,\n",
    "        protein_mean_length=300,\n",
    "        protein_std_length=50,\n",
    "        x_rate=0.0,\n",
    "        n_peptides=5000,\n",
    "        peptide_mean_length=10,\n",
    "        peptide_std_length=2,\n",
    "        match_fraction=0.0,\n",
    "        quasi_fraction=0.0,\n",
    "        redundancy_rate=0.0,\n",
    "        mutation_rate=0.0,\n",
    "        seed=4,\n",
    "    ),\n",
    "\n",
    "    \"repetitive\": dict(\n",
    "        n_proteins=2000,\n",
    "        protein_mean_length=300,\n",
    "        protein_std_length=50,\n",
    "        x_rate=0.0,\n",
    "        n_peptides=1000,\n",
    "        peptide_mean_length=7,\n",
    "        peptide_std_length=1,\n",
    "        match_fraction=0.8,\n",
    "        quasi_fraction=0.0,\n",
    "        redundancy_rate=0.3,\n",
    "        mutation_rate=0.05,\n",
    "        seed=5,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Register algorithms\n",
    "\n",
    "We retrieve all matching engines currently available in MicroTPCT.  \n",
    "The naive `str.find` implementation is used as a baseline reference for correctness and relative performance.\n",
    "\n",
    "All algorithms are evaluated using the same input databases and compared against a common ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_engine = \"find\"\n",
    "engines_to_test = list_available_engines()\n",
    "engines_to_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform benchmark\n",
    "\n",
    "For each scenario, we generate a fully controled synthetic target proteome and a set of query peptides.\n",
    "\n",
    "Each matching algorithm is then executed on the same input databases and evaluated using the following metrics:\n",
    "\n",
    "- Wall-clock execution time\n",
    "- CPU utilization\n",
    "- Peak resident memory consumption\n",
    "- Result correctness compared to the reference method (`str.find`)\n",
    "\n",
    "Each algorithm is executed 3 times and results are averaged.\n",
    "\n",
    "All benchmarks are executed sequentially in the same Python process in order to minimize variability due to process startup and I/O overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_func = get_engine(reference_engine)\n",
    "\n",
    "all_results: list[BenchmarkResult] = []\n",
    "\n",
    "for scenario_name, params in SCENARIOS.items():\n",
    "    print(f\"Running scenario: {scenario_name}\")\n",
    "\n",
    "    # Database generation\n",
    "    target_db, query_db, config = generate_benchmark_databases(**params)\n",
    "\n",
    "    reference = reference_func(target_db, query_db)\n",
    "\n",
    "    # Loop over selected matching engine (by default: all available)\n",
    "    for engine_name in engines_to_test:\n",
    "        print(f\"  Algorithm: {engine_name}\")\n",
    "\n",
    "        # Dynamically get the matching function\n",
    "        algo_func = get_engine(engine_name)\n",
    "\n",
    "        # Benchmark\n",
    "        res = run_benchmark(\n",
    "            algorithm_name=engine_name,\n",
    "            run_method=algo_func,\n",
    "            scenario_name=scenario_name,\n",
    "            target_db=target_db,\n",
    "            query_db=query_db,\n",
    "            reference=reference,\n",
    "            n_run=3\n",
    "        )\n",
    "\n",
    "        all_results.append(res)\n",
    "\n",
    "\n",
    "# Convert result to DataFrame\n",
    "\n",
    "df = pd.DataFrame([r.__dict__ for r in all_results])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sanity check: correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All algorithms are validated against the reference ground truth (made by `str.find`).  \n",
    "Any mismatch would indicate an implementation error or undefined behavior.\n",
    "\n",
    "This assertion guarantees that all reported performance metrics correspond to correct matching results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df[\"valid\"].all(), \"Some algorithms produced incorrect results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "We now visualize the benchmark results in order to compare algorithmic performance across scenarios.\n",
    "\n",
    "All figures are automatically exported and in order to be included in the MicroTPCT documentation and README.\n",
    "\n",
    "### 6.0 Configure figures saving parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURES_DIR = Path(\"figures\")\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def save_fig(name: str, dpi: int = 300):\n",
    "    path = FIGURES_DIR / f\"{_sanitize_name(name)}.png\"\n",
    "    plt.savefig(path, bbox_inches=\"tight\", dpi=dpi)\n",
    "    print(f\"Saved figure: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Execution time per algorithm and scenario\"\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df, x=\"algorithm\", y=\"wall_time\", hue=\"scenario\")\n",
    "plt.ylabel(\"Wall time (s)\")\n",
    "plt.title(title)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "save_fig(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Peak memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Peak memory consumption per algorithm and scenario\"\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df, x=\"algorithm\", y=\"peak_memory_mb\", hue=\"scenario\")\n",
    "plt.ylabel(\"Peak memory (MB)\")\n",
    "plt.title(title)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "save_fig(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 CPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"CPU utilization per algorithm and scenario\"\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df, x=\"algorithm\", y=\"cpu_utilization\", hue=\"scenario\")\n",
    "plt.ylabel(\"CPU utilization (%)\")\n",
    "plt.title(title)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "save_fig(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Speed–Memory trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title = \"Speed–Memory trade-off\"\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(data=df, x=\"peak_memory_mb\", y=\"wall_time\", hue=\"algorithm\", style=\"scenario\")\n",
    "plt.xlabel(\"Peak memory (MB)\")\n",
    "plt.ylabel(\"Wall time (s)\")\n",
    "plt.title(title)\n",
    "save_fig(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Scaling behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Wall time scaling with number of peptides\"\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=df, x=\"n_queries\", y=\"wall_time\", hue=\"algorithm\", style=\"scenario\", markers=True)\n",
    "plt.xlabel(\"Number of queries\")\n",
    "plt.ylabel(\"Wall time (s)\")\n",
    "plt.title(title)\n",
    "plt.tight_layout()\n",
    "save_fig(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Peak memory scaling with number of peptides\"\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=df, x=\"n_queries\", y=\"peak_memory_mb\", hue=\"algorithm\", style=\"scenario\", markers=True)\n",
    "plt.xlabel(\"Number of queries\")\n",
    "plt.ylabel(\"Peak memory (MB)\")\n",
    "plt.title(title)\n",
    "plt.tight_layout()\n",
    "save_fig(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key observations (to be completed)\n",
    "\n",
    "* Scenario: small_sparse\n",
    "\n",
    "  * [Placeholder: comment on relative speed and overhead]\n",
    "\n",
    "* Scenario: short_peptides\n",
    "\n",
    "  * [Placeholder: collision behavior, advantage of multi-pattern algorithms]\n",
    "\n",
    "* Scenario: negative_only\n",
    "\n",
    "  * [Placeholder: worst-case scanning cost]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = (\n",
    "    df.groupby([\"algorithm\"])\n",
    "      .agg({\n",
    "          \"wall_time\": \"mean\",\n",
    "          \"peak_memory_mb\": \"mean\",\n",
    "          \"cpu_user_time\": \"mean\",\n",
    "          \"valid\": \"all\",\n",
    "      })\n",
    "      .reset_index()\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Conclusion (template)\n",
    "\n",
    "This benchmark highlights several key findings:\n",
    "\n",
    "* The naive str.find implementation provides a reliable but slow baseline.\n",
    "* Multi-pattern algorithms (Aho–Corasick) scale better with increasing numbers of queries.\n",
    "* System-level tools (grep/awk) offer competitive performance with minimal memory overhead.\n",
    "* BLAST, while robust, is computationally overkill for exact peptide matching.\n",
    "\n",
    "Based on these results, we recommend:\n",
    "\n",
    "* Default algorithm: [TO FILL]\n",
    "* Use cases favoring alternative methods: [TO FILL]\n",
    "\n",
    "These results justify the default configuration of MicroTPCT and provide users\n",
    "with practical guidance for selecting an appropriate matching engine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
