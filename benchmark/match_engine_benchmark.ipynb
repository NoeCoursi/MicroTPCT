{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MicroTPCT – Matching Algorithm Benchmark Notebook\n",
    "\n",
    "This notebook benchmarks multiple exact matching algorithms for MicroTPCT.\n",
    "It measures execution time, CPU usage, peak memory consumption, and validates\n",
    "correctness against a controlled synthetic ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This benchmark evaluates several peptide-to-proteome exact matching strategies\n",
    "implemented in MicroTPCT. The objectives are:\n",
    "\n",
    "* Compare execution time, CPU usage, and memory consumption.\n",
    "* Verify correctness against a known ground truth.\n",
    "* Illustrate algorithmic behavior under different controlled scenarios.\n",
    "* Provide guidance for default algorithm selection and user-level configuration.\n",
    "\n",
    "Algorithms compared include:\n",
    "\n",
    "* Naive str.find baseline\n",
    "* Boyer-Moore\n",
    "* Aho-Corasick (Python)\n",
    "* Aho-Corasick (Rust)\n",
    "* grep + awk launcher\n",
    "* BLAST (reference but overkill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from microtpct.utils.data_generator import generate_benchmark_databases\n",
    "from microtpct.core.match import get_engine, list_available_engines\n",
    "from microtpct.io.writers import _sanitize_name\n",
    "\n",
    "from benchmark_core import BenchmarkResult, run_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Benchmark scenarios\n",
    "\n",
    "We define a small but illustrative set of scenarios designed to expose algorithmic differences without performing an exhaustive grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARIOS = {\n",
    "    \"small_sparse\": dict(\n",
    "        n_proteins=1000,\n",
    "        protein_mean_length=300,\n",
    "        protein_std_length=50,\n",
    "        x_rate=0.0,\n",
    "        n_peptides=1000,\n",
    "        peptide_mean_length=10,\n",
    "        peptide_std_length=2,\n",
    "        match_fraction=0.05,\n",
    "        quasi_fraction=0.0,\n",
    "        redundancy_rate=0.0,\n",
    "        mutation_rate=0.0,\n",
    "        seed=1,\n",
    "    ),\n",
    "\n",
    "    \"many_queries\": dict(\n",
    "        n_proteins=5000,\n",
    "        protein_mean_length=300,\n",
    "        protein_std_length=50,\n",
    "        x_rate=0.0,\n",
    "        n_peptides=20000,\n",
    "        peptide_mean_length=10,\n",
    "        peptide_std_length=2,\n",
    "        match_fraction=0.1,\n",
    "        quasi_fraction=0.0,\n",
    "        redundancy_rate=0.0,\n",
    "        mutation_rate=0.0,\n",
    "        seed=2,\n",
    "    ),\n",
    "\n",
    "    \"short_peptides\": dict(\n",
    "        n_proteins=2000,\n",
    "        protein_mean_length=300,\n",
    "        protein_std_length=50,\n",
    "        x_rate=0.0,\n",
    "        n_peptides=3000,\n",
    "        peptide_mean_length=6,\n",
    "        peptide_std_length=1,\n",
    "        match_fraction=0.3,\n",
    "        quasi_fraction=0.0,\n",
    "        redundancy_rate=0.0,\n",
    "        mutation_rate=0.0,\n",
    "        seed=3,\n",
    "    ),\n",
    "\n",
    "    \"negative_only\": dict(\n",
    "        n_proteins=5000,\n",
    "        protein_mean_length=300,\n",
    "        protein_std_length=50,\n",
    "        x_rate=0.0,\n",
    "        n_peptides=5000,\n",
    "        peptide_mean_length=10,\n",
    "        peptide_std_length=2,\n",
    "        match_fraction=0.0,\n",
    "        quasi_fraction=0.0,\n",
    "        redundancy_rate=0.0,\n",
    "        mutation_rate=0.0,\n",
    "        seed=4,\n",
    "    ),\n",
    "\n",
    "    \"repetitive\": dict(\n",
    "        n_proteins=2000,\n",
    "        protein_mean_length=300,\n",
    "        protein_std_length=50,\n",
    "        x_rate=0.0,\n",
    "        n_peptides=1000,\n",
    "        peptide_mean_length=7,\n",
    "        peptide_std_length=1,\n",
    "        match_fraction=0.8,\n",
    "        quasi_fraction=0.0,\n",
    "        redundancy_rate=0.3,\n",
    "        mutation_rate=0.05,\n",
    "        seed=5,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Register algorithms\n",
    "\n",
    "Text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_engine = \"find\"\n",
    "engines_to_test = list_available_engines()\n",
    "engines_to_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_func = get_engine(reference_engine)\n",
    "\n",
    "all_results: list[BenchmarkResult] = []\n",
    "\n",
    "for scenario_name, params in SCENARIOS.items():\n",
    "    print(f\"Running scenario: {scenario_name}\")\n",
    "\n",
    "    # Database generation\n",
    "    target_db, query_db, config = generate_benchmark_databases(**params)\n",
    "\n",
    "    reference = reference_func(target_db, query_db)\n",
    "\n",
    "    # Loop over selected matching engine (by default: all available)\n",
    "    for engine_name in engines_to_test:\n",
    "        print(f\"  Algorithm: {engine_name}\")\n",
    "\n",
    "        # Dynamically get the matching function\n",
    "        algo_func = get_engine(engine_name)\n",
    "\n",
    "        # Benchmark\n",
    "        res = run_benchmark(\n",
    "            algorithm_name=engine_name,\n",
    "            run_method=algo_func,\n",
    "            scenario_name=scenario_name,\n",
    "            target_db=target_db,\n",
    "            query_db=query_db,\n",
    "            reference=reference,\n",
    "        )\n",
    "\n",
    "        all_results.append(res)\n",
    "\n",
    "\n",
    "# Convert result to DataFrame\n",
    "\n",
    "df = pd.DataFrame([r.__dict__ for r in all_results])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sanity check: correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df[\"valid\"].all(), \"Some algorithms produced incorrect results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "### 6.0 Configure figures saving parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURES_DIR = Path(\"figures\")\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def save_fig(name: str, dpi: int = 300):\n",
    "    path = FIGURES_DIR / f\"{_sanitize_name(name)}.png\"\n",
    "    plt.savefig(path, bbox_inches=\"tight\", dpi=dpi)\n",
    "    print(f\"Saved figure: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Execution time per algorithm and scenario\"\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df, x=\"algorithm\", y=\"wall_time\", hue=\"scenario\")\n",
    "plt.ylabel(\"Wall time (s)\")\n",
    "plt.title(title)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "save_fig(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Peak memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Peak memory consumption per algorithm and scenario\"\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df, x=\"algorithm\", y=\"peak_memory_mb\", hue=\"scenario\")\n",
    "plt.ylabel(\"Peak memory (MB)\")\n",
    "plt.title(title)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "save_fig(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Processor utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"CPU utilization per algorithm and scenario\"\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df, x=\"algorithm\", y=\"cpu_utilization\", hue=\"scenario\")\n",
    "plt.ylabel(\"CPU utilization (%)\")\n",
    "plt.title(title)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "save_fig(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Scaling behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Scaling with number of peptides\"\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=df, x=\"n_queries\", y=\"wall_time\", hue=\"algorithm\", style=\"scenario\", markers=True)\n",
    "plt.xlabel(\"Number of queries\")\n",
    "plt.ylabel(\"Wall time (s)\")\n",
    "plt.title(title)\n",
    "plt.tight_layout()\n",
    "save_fig(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Scaling with number of peptides\"\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=df, x=\"n_queries\", y=\"peak_memory_mb\", hue=\"algorithm\", style=\"scenario\", markers=True)\n",
    "plt.xlabel(\"Number of queries\")\n",
    "plt.ylabel(\"Peak memory (MB)\")\n",
    "plt.title(title)\n",
    "plt.tight_layout()\n",
    "save_fig(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"wall_time\", \"cpu_utilization\", \"peak_memory_mb\"]:\n",
    "    df[f\"norm_{metric}\"] = (\n",
    "        df.groupby(\"scenario\")[metric]\n",
    "          .transform(lambda x: x / x.min())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = df.pivot(index=\"algorithm\", columns=\"scenario\", values=\"norm_cpu_utilization\")\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(pivot, annot=True, cmap=\"magma\")\n",
    "plt.title(\"Normalised CPU Usage\")\n",
    "save_fig(\"heatmap_cpu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = df.pivot(index=\"algorithm\", columns=\"scenario\", values=\"norm_peak_memory_mb\")\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(pivot, annot=True, cmap=\"cividis\")\n",
    "plt.title(\"Normalised Peak RAM Usage\")\n",
    "save_fig(\"heatmap_ram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key observations (to be completed)\n",
    "\n",
    "* Scenario: small_sparse\n",
    "\n",
    "  * [Placeholder: comment on relative speed and overhead]\n",
    "\n",
    "* Scenario: short_peptides\n",
    "\n",
    "  * [Placeholder: collision behavior, advantage of multi-pattern algorithms]\n",
    "\n",
    "* Scenario: negative_only\n",
    "\n",
    "  * [Placeholder: worst-case scanning cost]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = (\n",
    "    df.groupby([\"algorithm\"])\n",
    "      .agg({\n",
    "          \"wall_time\": \"mean\",\n",
    "          \"peak_memory_mb\": \"mean\",\n",
    "          \"cpu_user_time\": \"mean\",\n",
    "          \"valid\": \"all\",\n",
    "      })\n",
    "      .reset_index()\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Conclusion (template)\n",
    "\n",
    "This benchmark highlights several key findings:\n",
    "\n",
    "* The naive str.find implementation provides a reliable but slow baseline.\n",
    "* Multi-pattern algorithms (Aho–Corasick) scale better with increasing numbers of queries.\n",
    "* System-level tools (grep/awk) offer competitive performance with minimal memory overhead.\n",
    "* BLAST, while robust, is computationally overkill for exact peptide matching.\n",
    "\n",
    "Based on these results, we recommend:\n",
    "\n",
    "* Default algorithm: [TO FILL]\n",
    "* Use cases favoring alternative methods: [TO FILL]\n",
    "\n",
    "These results justify the default configuration of MicroTPCT and provide users\n",
    "with practical guidance for selecting an appropriate matching engine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
